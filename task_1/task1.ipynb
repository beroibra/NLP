{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_list = [\"label\", \"text\"]\n",
    "input = pd.read_csv('../SMS_Spam_Collection/SMSSpamCollection', delimiter = \"\\t\",names = column_names_list)\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing the length of texts in each class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = input.copy(deep=True)\n",
    "data[\"text length\"] = data[\"text\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"label\"]== \"ham\"][\"text length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"label\"]== \"ham\"][\"text length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(8.1, 5))\n",
    "\n",
    "sns.histplot(data[data[\"label\"]== \"ham\"][\"text length\"], color=\"green\",ax=axs[0],stat=\"density\")\n",
    "axs[0].set_xlabel('Message Length')\n",
    "axs[0].set_title('Ham Messages')\n",
    "axs[0].set_xticks(np.arange(801, step=100))\n",
    "sns.histplot(data[data[\"label\"]== \"spam\"][\"text length\"], color=\"red\",ax=axs[1],stat=\"density\")\n",
    "axs[1].set_xlabel('Message Length')\n",
    "axs[1].set_ylabel('')\n",
    "axs[1].set_title('Spam Messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,sharey='row')\n",
    "data[data[\"label\"]== \"ham\"][\"text length\"].hist(bins=20, edgecolor = 'black',ax=axes[0], density=True,color=\"blue\")\n",
    "data[data[\"label\"]== \"spam\"][\"text length\"].hist(bins=20, edgecolor = 'black',ax=axes[1], density=True,color=\"red\")\n",
    "fig.suptitle('Message lengths in each class', fontsize=16,y=1)\n",
    "axes[0].set_xlabel('Message Length')\n",
    "axes[0].set_ylabel('density')\n",
    "axes[1].set_xlabel('Message Length')\n",
    "axes[1].set_ylabel('density')\n",
    "axes[0].set_title('Ham')\n",
    "axes[1].set_title('Spam')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyzing word statistics ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleaning\n",
    "analysis2 = data.copy(deep=True)\n",
    "analysis2[\"text\"] = analysis2[\"text\"].str.replace('&',\"\").str.replace('/',\"\").str.replace('>',\"\").str.replace('<',\"\").str.replace('\"',\"\").str.replace(\"''\",\"\").str.replace(\"!\",\"\").str.replace(\"?\",\"\").str.replace(\".\",\"\").str.replace(\",\",\"\").str.replace(\":\",\"\").str.replace(\";\",\"\").str.replace(\"*\",\"\").str.replace(\"#\",\"\").str.replace(\"£\",\"\").str.replace(r\"[0-9]\",\"\",regex=True).str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = analysis2[analysis2[\"label\"] == \"spam\"][\"text\"].str.split(\" \")\n",
    "ham_words = analysis2[analysis2[\"label\"] == \"ham\"][\"text\"].str.split(\" \")\n",
    "# all words in spam messagses\n",
    "spam_words_concatenated = []\n",
    "for x in spam_words: \n",
    "    spam_words_concatenated = spam_words_concatenated + x\n",
    "# all words in ham messagses\n",
    "ham_words_concatenated = []\n",
    "for x in ham_words: \n",
    "    ham_words_concatenated = ham_words_concatenated + x\n",
    "# create a series containing the words in each class\n",
    "ham_words_concatenated = pd.Series(ham_words_concatenated).str.strip().replace('', np.nan).dropna()\n",
    "spam_words_concatenated = pd.Series(spam_words_concatenated).str.strip().replace('', np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_words = list(set(list(unwanted_words) + [\"one\",\"lor\",\"about\",\"can\",\"have\",\"-\",\"only\",\"just\",\"+\",\"p\",\"our\",\"now\",\"from\",\"going\",\"i'll\",\"ü\",\"he\",\"there\",\"do\",\"was\",\"its\",\"then\",\"how\",\"am\",\"with\",\"or\",\"ok\",\"no\",\"this\",\"what\",\"when\",\"we\",\"ur\",\"ltgt\",\"be\",\"if\",\"i'm\",'i', 'you', 'to', 'the', 'a', 'u', 'and', 'in', 'me', 'my','is','it','of','for','that','but','your','so','not','are','on','at']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printing most frequent words in each class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 20 most frequent words in the spam messages after deleting pronouns, wh-questions etc.. are: \",list(spam_words_concatenated.value_counts(normalize=True)[~spam_words_concatenated.value_counts().index.isin(unwanted_words)].head(20).index) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 20 most frequent words in the ham messages after deleting pronouns, wh-questions etc.. are: \",list(ham_words_concatenated.value_counts(normalize=True)[~ham_words_concatenated.value_counts().index.isin(unwanted_words)].head(20).index) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating the average word-length in each class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the average word-length in the ham class is: \", round(ham_words_concatenated.apply(len).mean(),ndigits=2))\n",
    "print(\"The average word-length after deleting pronouns, wh-questions etc.. is: \",round(ham_words_concatenated[~ham_words_concatenated.isin(unwanted_words)].apply(len).mean(),ndigits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the average word-length in the spam class is: \", round(spam_words_concatenated.apply(len).mean(),ndigits=2))\n",
    "print(\"The average word-length after deleting pronouns, wh-questions etc.. is: \",round(spam_words_concatenated[~spam_words_concatenated.isin(unwanted_words)].apply(len).mean(),ndigits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of unique words in each class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of unique words in the ham class is: \",len(ham_words_concatenated.drop_duplicates()))\n",
    "print(\"The number of unique words in the spam class is: \",len(spam_words_concatenated.drop_duplicates()))\n",
    "print(\"The number of unique words in the whole dataset is: \",len(pd.concat([spam_words_concatenated,ham_words_concatenated]).drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multidict as multidict\n",
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "def getFrequencyDictForText(sentence):\n",
    "    fullTermsDict = multidict.MultiDict()\n",
    "    tmpDict = {}\n",
    "\n",
    "    # making dict for counting frequencies\n",
    "    for text in sentence.split(\" \"):\n",
    "        if re.match(\"a|the|an|the|to|in|for|of|or|by|with|is|on|that|be\", text):\n",
    "            continue\n",
    "        val = tmpDict.get(text, 0)\n",
    "        tmpDict[text.lower()] = val + 1\n",
    "    for key in tmpDict:\n",
    "        fullTermsDict.add(key, tmpDict[key])\n",
    "    return fullTermsDict\n",
    "\n",
    "\n",
    "def makeImage(text):\n",
    "\n",
    "    wc = WordCloud(width=1600, height=800,background_color=\"white\", max_words=1000)\n",
    "    # generate word cloud\n",
    "    wc.generate_from_frequencies(text)\n",
    "\n",
    "    # show\n",
    "    plt.figure( figsize=(9,5))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequency image for words in ham sms\n",
    "makeImage(getFrequencyDictForText(ham_words_concatenated[~ham_words_concatenated.isin(unwanted_words)].str.cat(sep=\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequency image for words in spam sms\n",
    "makeImage(getFrequencyDictForText(spam_words_concatenated[~spam_words_concatenated.isin(unwanted_words)].str.cat(sep=\" \")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
