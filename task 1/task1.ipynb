{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_list = [\"label\", \"text\"]\n",
    "input = pd.read_csv('SMS_Spam_Collection/SMSSpamCollection', delimiter = \"\\t\",names = column_names_list)\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doing some analysis using a copy of the corpus ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing the length of texts in each class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = input.copy(deep=True)\n",
    "data[\"text length\"] = data[\"text\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"label\"]== \"ham\"][\"text length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"label\"]== \"ham\"][\"text length\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,sharey='row')\n",
    "data[data[\"label\"]== \"ham\"][\"text length\"].hist(bins=20, edgecolor = 'black',ax=axes[0], density=True)\n",
    "data[data[\"label\"]== \"spam\"][\"text length\"].hist(bins=20, edgecolor = 'black',ax=axes[1], density=True)\n",
    "fig.suptitle('Message lengths in each class', fontsize=16,y=1)\n",
    "axes[0].set_xlabel('Message Length')\n",
    "axes[0].set_ylabel('density')\n",
    "axes[1].set_xlabel('Message Length')\n",
    "axes[1].set_ylabel('density')\n",
    "axes[0].set_title('Ham')\n",
    "axes[1].set_title('Spam')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyzing word statistics ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleaning\n",
    "analysis2 = data.copy(deep=True)\n",
    "analysis2[\"text\"] = analysis2[\"text\"].str.replace('&',\"\").str.replace('/',\"\").str.replace('>',\"\").str.replace('<',\"\").str.replace('\"',\"\").str.replace(\"''\",\"\").str.replace(\"!\",\"\").str.replace(\"?\",\"\").str.replace(\".\",\"\").str.replace(\",\",\"\").str.replace(\":\",\"\").str.replace(\";\",\"\").str.replace(\"*\",\"\").str.replace(\"#\",\"\").str.replace(\"£\",\"\").str.replace(r\"[0-9]\",\"\",regex=True).str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = analysis2[analysis2[\"label\"] == \"spam\"][\"text\"].str.split(\" \")\n",
    "ham_words = analysis2[analysis2[\"label\"] == \"ham\"][\"text\"].str.split(\" \")\n",
    "# all words in spam messagses\n",
    "spam_words_concatenated = []\n",
    "for x in spam_words: \n",
    "    spam_words_concatenated = spam_words_concatenated + x\n",
    "# all words in ham messagses\n",
    "ham_words_concatenated = []\n",
    "for x in ham_words: \n",
    "    ham_words_concatenated = ham_words_concatenated + x\n",
    "# create a series containing the words in each class\n",
    "ham_words_concatenated = pd.Series(ham_words_concatenated).str.strip().replace('', np.nan).dropna()\n",
    "spam_words_concatenated = pd.Series(spam_words_concatenated).str.strip().replace('', np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_words = [\"one\",\"lor\",\"about\",\"can\",\"have\",\"-\",\"only\",\"just\",\"+\",\"p\",\"our\",\"now\",\"from\",\"going\",\"i'll\",\"ü\",\"he\",\"there\",\"do\",\"was\",\"its\",\"then\",\"how\",\"am\",\"with\",\"or\",\"ok\",\"no\",\"this\",\"what\",\"when\",\"we\",\"ur\",\"ltgt\",\"be\",\"if\",\"i'm\",'i', 'you', 'to', 'the', 'a', 'u', 'and', 'in', 'me', 'my','is','it','of','for','that','but','your','so','not','are','on','at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printing most frequent words in each class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 20 most frequent words in the spam messages after deleting pronouns, wh-questions etc.. are: \",list(spam_words_concatenated.value_counts(normalize=True)[~spam_words_concatenated.value_counts().index.isin(unwanted_words)].head(20).index) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 20 most frequent words in the ham messages after deleting pronouns, wh-questions etc.. are: \",list(ham_words_concatenated.value_counts(normalize=True)[~ham_words_concatenated.value_counts().index.isin(unwanted_words)].head(20).index) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating the average word-length in each class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the average word-length in the ham class is: \", round(ham_words_concatenated.apply(len).mean(),ndigits=2))\n",
    "print(\"The average word-length after deleting pronouns, wh-questions etc.. is: \",round(ham_words_concatenated[~ham_words_concatenated.isin(unwanted_words)].apply(len).mean(),ndigits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the average word-length in the spam class is: \", round(spam_words_concatenated.apply(len).mean(),ndigits=2))\n",
    "print(\"The average word-length after deleting pronouns, wh-questions etc.. is: \",round(spam_words_concatenated[~spam_words_concatenated.isin(unwanted_words)].apply(len).mean(),ndigits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of unique words in each class ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of unique words in the ham class is: \",len(ham_words_concatenated.drop_duplicates()))\n",
    "print(\"The number of unique words in the spam class is: \",len(spam_words_concatenated.drop_duplicates()))\n",
    "print(\"The number of unique words in the whole dataset is: \",len(pd.concat([spam_words_concatenated,ham_words_concatenated]).drop_duplicates()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
