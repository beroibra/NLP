{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gensim numpy pandas nltk textblob scipy sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMSsRjONrYMn",
        "outputId": "f20a5713-cfa0-4eda-ed6f-5000133d5a39"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf_Rjs0Msrhm",
        "outputId": "afda22b0-217b-4bed-fec6-dffb96193aa6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-29 13:42:24--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 681808098 (650M) [application/zip]\n",
            "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650.22M  22.7MB/s    in 31s     \n",
            "\n",
            "2022-06-29 13:42:56 (20.9 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/wiki-news-300d-1M.vec.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoMBnxh5tDHZ",
        "outputId": "fe89e619-a1c4-4c28-d7d1-38b06c6c6e2e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/wiki-news-300d-1M.vec.zip\n",
            "  inflating: wiki-news-300d-1M.vec   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format('/content/wiki-news-300d-1M.vec')"
      ],
      "metadata": {
        "id": "Et1ZmmkvtWG6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2word_set = set(model.wv.index2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkETLQ0vtfXM",
        "outputId": "c011d5c8-b876-42a5-eabf-68736125ddb9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordPunctTokenizer\n",
        "from textblob.base import BaseTokenizer\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import pickle\n",
        "\n",
        "\n",
        "class NLTKWordPunctTokenizer(BaseTokenizer):\n",
        "    def tokenize(self, text):\n",
        "        return WordPunctTokenizer().tokenize(text)\n",
        "\n",
        "\n",
        "def avg_feature_vector(sentence, num_features=300):\n",
        "    words = NLTKWordPunctTokenizer().tokenize(sentence)\n",
        "    feature_vec = np.zeros((num_features,), dtype='float32')\n",
        "    n_words = 0\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            n_words += 1\n",
        "            feature_vec = np.add(feature_vec, model[word])\n",
        "    if (n_words > 0):\n",
        "        feature_vec = np.divide(feature_vec, n_words)\n",
        "    return feature_vec\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return 1 - spatial.distance.cosine(vec1, vec2)\n",
        "\n",
        "\n",
        "def sent2sent_similarity(sentence1, sentence2):\n",
        "    vec1, vec2 = avg_feature_vector(sentence1), avg_feature_vector(sentence2)\n",
        "    return cosine_similarity(vec1, vec2)\n"
      ],
      "metadata": {
        "id": "cLThAgZ7ySem"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"No message..no responce..what happend?\",\n",
        "             \"At WHAT TIME should i come tomorrow\",\n",
        "             \"Come to my home for one last time i wont do anything. Trust me.\",\n",
        "             \"See you there!\",\n",
        "             \"Great. So should i send you my account number.\",\n",
        "             \"Do we have any spare power supplies\",\n",
        "             \"Ok try to do week end course in coimbatore.\",\n",
        "             \"I have lost 10 kilos as of today!\",\n",
        "             \"Still chance there. If you search hard you will get it..let have a try :)\",\n",
        "             \"Night night, see you tomorrow\",\n",
        "             \"Get ur 1st RINGTONE FREE NOW! Reply to this msg with TONE. Gr8 TOP 20 tones to your phone every week just £1.50 per wk 2 opt out send STOP 08452810071 16\",\n",
        "             \"URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\",\n",
        "             \"YES! The only place in town to meet exciting adult singles is now in the UK. Txt CHAT to 86688 now! 150p/Msg.\",\n",
        "             \"URGENT!: Your Mobile No. was awarded a £2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9755 BOX95QU\",\n",
        "             \"Ever thought about living a good life with a perfect partner? Just txt back NAME and AGE to join the mobile community. (100p/SMS)\",\n",
        "             \"We tried to contact you re your reply to our offer of a Video Phone 750 anytime any network mins Half Price Line Rental Camcorder Reply or call 08000930705\",\n",
        "             \"Free Top ringtone -sub to weekly ringtone-get 1st week free-send SUBPOLY to 81618-?3 per week-stop sms-08718727870\",\n",
        "             \"Your weekly Cool-Mob tones are ready to download !This weeks new Tones include: 1) Crazy Frog-AXEL F>>> 2) Akon-Lonely>>> 3) Black Eyed-Dont P >>>More info in n\",\n",
        "             \"Bought one ringtone and now getting texts costing 3 pound offering more tones etc\",\n",
        "             \"URGENT We are trying to contact you Last weekends draw shows u have won a £1000 prize GUARANTEED Call 09064017295 Claim code K52 Valid 12hrs 150p pm\"]\n",
        "labels = [\"ham\"] * 10 + [\"spam\"] * 10\n",
        "\n",
        "sentence_embeddings = list(map(lambda x: avg_feature_vector(x), sentences))\n",
        "\n",
        "with open('sentence_embeddings.pickle', 'wb') as handle:\n",
        "    pickle.dump(sentence_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "pairwise_cosine_dists = pairwise_distances(sentence_embeddings, metric=cosine_similarity)\n",
        "pairwise_cosine_dists.shape\n",
        "pairwise_cosine_dists\n",
        "\n",
        "with open('pairwise_cosine_dists.pickle', 'wb') as handle:\n",
        "    pickle.dump(pairwise_cosine_dists, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "tk3MhiUduHYG"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}